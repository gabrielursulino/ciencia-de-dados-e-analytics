{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielursulino/ciencia-de-dados-e-analytics/blob/main/mvp_machine_learning_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL0aCOIDqYrR"
      },
      "source": [
        "# MVP: Machine Learning & Analytics\n",
        "**Nome:** Gabriel Lopes Ursulino\n",
        "\n",
        "**Matrícula:** 4052025000406\n",
        "\n",
        "**Dataset original:** [Cerebral Stroke Prediction-Imbalanced Dataset](https://www.kaggle.com/datasets/shashwatwork/cerebral-stroke-predictionimbalaced-dataset/data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9uR1x82sscx"
      },
      "source": [
        "# Descrição do Problema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJbEQ7FmqlvR"
      },
      "source": [
        "O Acidente Vascular Cerebral (AVC), também conhecido pelo termo em inglês *stroke*, é uma das principais causas de morte e incapacidade no mundo. O AVC acontece quando vasos que levam sangue ao cérebro entopem ou se rompem, provocando a paralisia da área cerebral que ficou sem circulação sanguínea. O AVC pode ser hemorrágico, quando há rompimento de um vaso cerebral, provocando hemorragia, ou isquêmico, quando há obstrução de uma artéria, impedindo a passagem de oxigênio para células cerebrais, que acabam morrendo. Essa obstrução pode acontecer devido a um trombo (trombose) ou a um êmbolo (embolia).\n",
        "\n",
        "O conjunto de dados *Cerebral Stroke Prediction-Imbalanced Dataset* é um conjunto de dados multivariado que reúne diversas características que podem estar associadas ao risco de AVC, como idade, hábitos, estilo de vida e condições prévias de saúde como hipertensão e doenças cardiovasculares.\n",
        "\n",
        "O objetivo deste MVP é dar continuidade no trabalho desenvolvido na Sprint anterior e desenvolver um modelo de Machine Learning capaz de classificar os pacientes com risco de sofrer um AVC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woLrtWdOsDFj"
      },
      "source": [
        "## Hipóteses do Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUrr1ut8IN0U"
      },
      "source": [
        "É possível, a partir dos dados extremamente desbalanceados e com escassez de variáveis de entrada, construir um modelo modelo de classificação robusto e confiável para a detecção de ocorrência de AVCs afim de fornecer insumos para estratégias de prevenção?\n",
        "\n",
        "As técnicas de pré-processamento e balanceamento são capazes de oferecer melhorias significativas quando comparados a um modelo simples sem qualquer técnica aplicada?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcwU_P4-sbar"
      },
      "source": [
        "## Tipo de Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mJDs5PyRfRT"
      },
      "source": [
        "Este é um problema de classificação supervisionada. Dado um conjunto de características (idade, genero, hipertenso, fumante etc.), o objetivo é avaliar os fatores de risco que contribuem para ocorrência de AVC para prever futuras ocorrências."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1p8QScjsf8c"
      },
      "source": [
        "## Seleção de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV-0gxOMUjqy"
      },
      "source": [
        "O dataset original é amplamente disponível. Não é necessária uma etapa de seleção de dados externa, pois o dataset já está curado e pronto para uso. No entanto, o dataset apresentado é uma versão que já passou por uma análise exploratória e algumas etapas de pré-processamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsRzjZOdzIQo"
      },
      "source": [
        "## Atributos do Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgC1cqATUif8"
      },
      "source": [
        "\n",
        "O dataset contém 43.389 amostras, com os seguintes atributos:\n",
        "\n",
        "- **`gender`** (Gênero: \"Male\", \"Female\" or \"Other\"), em português: \"masculino\", \"feminino\" ou \"outro\", respectivamente.\n",
        "- **`age`** (Idade do paciente em anos)\n",
        "- **`hypertension`** (hipertensão: 0 se o paciente não possui, 1 se o paciente possui)\n",
        "- **`heart_disease`** (doença cardiovascular: 0 se o pacitente não possui, 1 se o paciente possui)\n",
        "- **`ever_married`** (Se o paciente já foi casado: \"Yes\" or \"No\") em português : \"sim\" ou \"não\", respectivamente.\n",
        "- **`Residence_type`** (Local de moradia: \"Rural\" or \"Urban\") em português: \"rural\" ou \"urbano\", respectivamente.\n",
        "- **`avg_glucose_level`** (Nível médio de glicose no sangue do paciente em mg/dL)\n",
        "- **`bmi`** (Body Mass Index, ou Índice de Massa Corporal (IMC))\n",
        "- **`stroke`** (AVC: 0 se o paciente não teve AVC, 1 se o paciente teve)\n",
        "- **`work_type_Never_worked`**, **`work_type_Private`**, **`work_type_Self-employed`**, **`work_type_children`**  (Colunas geradas a partir do One-Hot Encoding da coluna **`work_type`** do dataset original. Informa se o tipo de trabalho é do setor privado, autônomo, se nunca trabalhou etc.)\n",
        "- **`smoking_status_never smoked`**, **`smoking_status_smokes`**, **`smoking_status_unknown`** (Colunas geradas a partir do One-Hot Encoding da coluna **`smoking_status`** do dataset original. Informa o status de tabagismo, **`never smoked`** = nunca fumou, **`formerly smoked`** = ex-fumante, **`smokes`** = fumante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdY0TWwAzg8_"
      },
      "source": [
        "# Importação das Bibliotecas Necessárias e Carga de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khL0EMgEBFrV"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas e reprodutibilidade\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier # Ensembles\n",
        "from sklearn.metrics import accuracy_score, precision_recall_curve, make_scorer, recall_score, precision_score, classification_report, roc_auc_score, confusion_matrix # Para a exibição das métricas\n",
        "from sklearn.neighbors import KNeighborsClassifier # algoritmo KNN\n",
        "from sklearn.tree import DecisionTreeClassifier # algoritmo Árvore de Classificação\n",
        "from sklearn.naive_bayes import GaussianNB # algoritmo Naive Bayes\n",
        "from sklearn.svm import SVC # algoritmo SVM\n",
        "from sklearn.linear_model import LogisticRegression #algoritmo Regressão Logística\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import class_weight\n",
        "import time # Para medição do tempo de treino\n",
        "\n",
        "\n",
        "# Reprodutibilidade\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLcJW9zWBHqL"
      },
      "outputs": [],
      "source": [
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/gabrielursulino/ciencia-de-dados-e-analytics/refs/heads/main/datasets/stroke_dataset_preproc.csv\"\n",
        "\n",
        "# Lê o arquivo\n",
        "avcpreproc = pd.read_csv(url, delimiter=',')\n",
        "\n",
        "# Exibe as primeiras linhas do novo DataFrame\n",
        "avcpreproc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9haw3UmHvDq"
      },
      "source": [
        "*Para mais detalhes a respeito do dataset original, acesse:* https://www.kaggle.com/datasets/shashwatwork/cerebral-stroke-predictionimbalaced-dataset/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehmhwnOHzK5"
      },
      "source": [
        "#Informações Gerais e Resumo da AED e Pré-processamento (realizados na Sprint anterior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPKtil1H4P5n"
      },
      "source": [
        "Na Sprint anterior, durante a etapa de Análise Exploratória foi identificada a necessidade de tratamento dos dados ausentes nas colunas ***bmi*** e ***smoking_status***.\n",
        "\n",
        "A coluna ***bmi*** apresentou valores nulos em aproximadamente 3.37% dos registros e a análise dos boxplots revelou a presença de outliers. A estratégia de imputação escolhida foi preencher estes valores ausentes com a mediana, por ser uma medida mais robusta a valores extremos e, portanto, a mais indicada para não distorcer a distribuição original dos dados.\n",
        "\n",
        "A coluna ***smoking_status*** apresentou uma quantidade massiva de linhas vazias, correspondendo a mais de 30% do dataset. Diante de um volume tão grande, a imputação com a moda foi descartada por introduzir um viés significativo nos dados.  A exclusão das linhas acarretaria em uma grande perda de informação e a exclusão da coluna também não foi considerada, visto que identificamos o status de tabagismo como um fator de risco para ocorrência de AVC.\n",
        "\n",
        "Logo, a decisão estratégica foi imputar os valores vazios desta coluna com uma nova categoria, ***'unknown'***. Essa abordagem tem a dupla vantagem de preservar todas as instâncias do dataset e, ao mesmo tempo, transformar a falta de informação em uma feature que o próprio modelo de Machine Learning poderá usar para determinar se a ausência de dados sobre tabagismo é, por si só, um fator preditivo.\n",
        "\n",
        "Embora tenha sido localizada a presença de diversos outliers, principalmente em ***avg_glucose_level*** (Nível de Glicose) e ***bmi*** (IMC), não se tratam de erros de digitação ou medição. São medidas válidas e extremamente valiosas para o modelo de predição. Por essa razão, a decisão estratégica é não fazer nenhum tipo de tratamento e manter os dados originais.\n",
        "\n",
        "Ainda na etapa de pré-processamento, foi realizada a conversão das variáveis categóricas ***gender***, ***ever_married*** e ***Residence_type*** em valores binários e utilizei One-Hot Encoding para tratamento das colunas ***work_type*** e ***smoking_status***.\n",
        "\n",
        "Em resumo, após extensa análise e tratamento, o dataset está pronto que o modelo seja trabalhado.\n",
        "\n",
        "*Para mais detalhes a respeito do trabalho que foi desenvolvido, clique no link a seguir:* https://github.com/gabrielursulino/ciencia-de-dados-e-analytics/blob/main/MVP/mvp_analise_de_dados_e_boas_praticas.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lxd8PCrFxfH"
      },
      "outputs": [],
      "source": [
        "# Verificando as dimensões\n",
        "print(f\"Dimensões do Dataset: {avcpreproc.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Verificando os tipos de dados e valores não-nulos\n",
        "print(\"Informações Gerais e Tipos de Dados:\")\n",
        "avcpreproc.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3jcRtDete6A"
      },
      "source": [
        "O dataset que será trabalhado possui 43.389 instâncias e 16 colunas. Temos 3 variáveis do tipo *float* e 13 do tipo *int*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rit0F9qWZ4b"
      },
      "outputs": [],
      "source": [
        "# Informação em % da distribuição\n",
        "print(\"Proporção das classes em 'stroke' em %:\")\n",
        "print(avcpreproc['stroke'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Gráfico de barras para análise da variável alvo\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(x='stroke', hue='stroke', data=avcpreproc, palette={0: '#ADD8E6', 1: '#FF0000'}, legend=False)\n",
        "plt.title('Distribuição da ocorrência de AVC')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Contagem')\n",
        "plt.xticks(ticks=[0, 1], labels=['Não teve AVC (0)', 'Teve AVC (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-_CZ-NVVG3U"
      },
      "source": [
        "Com esse gráfico podemos confirmar que se trata de um dataset extremamente desbalanceado em termos de classes (98,20% contra 1,80%).  Isso exigirá balanceamento para evitar que o modelo seja enviesado para a classe majoritária."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOw4nNZQ1Z67"
      },
      "source": [
        "# Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF_ibGiDnG6W"
      },
      "source": [
        "##Separação entre Treino e Teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNCIabxQ2CBn"
      },
      "source": [
        "No código abaixo optei em dividir o conjunto entre 70/30, garantindo ao modelo um volume de dados robusto para tentar aprender os complexos padrões subjacentes aos fatores de risco de um AVC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD-CWCefgo3N"
      },
      "outputs": [],
      "source": [
        "# Separação dos Dados (Features e Target)\n",
        "X = avcpreproc.drop('stroke', axis=1)\n",
        "y = avcpreproc['stroke']\n",
        "\n",
        "# Divisão estratificada em conjuntos de treino e teste para manter a proporção\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3, random_state=SEED, stratify = y)\n",
        "\n",
        "print('Tamanho das separações:')\n",
        "print(f\"Treino: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Teste: {X_test.shape}, {y_test.shape}\")\n",
        "print(f\"Proporção de stroke no treino: {y_train.mean():.4f}\")\n",
        "print(f\"Proporção de stroke no teste: {y_test.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgokVaB4nK8w"
      },
      "source": [
        "##Padronização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo373Cja5TTJ"
      },
      "source": [
        "Devido à existência de outliers e às distribuições assimétricas, optei pela padronização. A normalização comprime as distribuições e um único outlier poderia achatar a escala dos dados restantes. Com a padronização eu evito esse problema, pois ela é mais robusta aos outliers ao utilizar média e desvio padrão para cálculo, ao invés de mínimo e máximo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir dos histrogramas abaixo, podemos verificar a distribuição dos dados no dataset."
      ],
      "metadata": {
        "id": "BMIkZ3lKcXpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirguração e título\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "plt.suptitle('Histograma das Variáveis Contínuas', fontsize=16)\n",
        "\n",
        "# Histograma 1: age (idade)\n",
        "sns.histplot(ax=axes[0], data=avcpreproc, x='age', kde=True, color='lightgreen')\n",
        "axes[0].set_ylabel('Frequência')\n",
        "axes[0].set_xlabel('Distribuição de Idade')\n",
        "\n",
        "# Histograma 2: avg_glucose_level (nível de glicose)\n",
        "sns.histplot(ax=axes[1], data=avcpreproc, x='avg_glucose_level', kde=True, color='lightblue')\n",
        "axes[1].set_ylabel('Frequência')\n",
        "axes[1].set_xlabel('Nível Médio de Glicose')\n",
        "\n",
        "# Histograma 3: bmi (IMC)\n",
        "sns.histplot(ax=axes[2], data=avcpreproc, x='bmi', kde=True, color='red')\n",
        "axes[2].set_ylabel('Frequência')\n",
        "axes[2].set_xlabel('IMC')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FsKxFNpbcZ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOcqIrPBZjYg"
      },
      "source": [
        "Abaixo será feita a padronização dos dados dessas colunas utilizando o ColumnTransformer para posterior adição aos pipelines dos testes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quais colunas serão padronizadas\n",
        "vcontinuas = ['age', 'avg_glucose_level', 'bmi']\n",
        "\n",
        "# Pipeline de pré-processamento focado na padronização\n",
        "preprocessor = ColumnTransformer(transformers=[('scaler_continuas', StandardScaler(), vcontinuas)],remainder='passthrough')\n",
        "\n",
        "# Aplica o pré-processador\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Separa as colunas que não foram padronizadas\n",
        "passthrough_cols = [col for col in X_train.columns if col not in vcontinuas]\n",
        "processed_cols = vcontinuas + passthrough_cols\n",
        "\n",
        "# Converte e define dataframe que será usado como conjunto de treino\n",
        "X_train_scaled = pd.DataFrame(X_train_processed, columns=processed_cols)"
      ],
      "metadata": {
        "id": "hUcOIuFtceos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exibe as primeiras linhas dos dados de treino padronizados\n",
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "7N4egsEkcozu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirguração e título dos histogramas padronizados\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "plt.suptitle('Histograma das Variáveis Contínuas Padronizadas', fontsize=16)\n",
        "\n",
        "# Histograma padronizado 1: age (idade)\n",
        "sns.histplot(ax=axes[0], data=X_train_scaled, x='age', kde=True, color='lightgreen')\n",
        "axes[0].set_ylabel('Frequência')\n",
        "axes[0].set_xlabel('Distribuição de Idade')\n",
        "\n",
        "# Histograma padronizado 2: avg_glucose_level (nível de glicose)\n",
        "sns.histplot(ax=axes[1], data=X_train_scaled, x='avg_glucose_level', kde=True, color='lightblue')\n",
        "axes[1].set_ylabel('Frequência')\n",
        "axes[1].set_xlabel('Nível Médio de Glicose')\n",
        "\n",
        "# Histograma 3: bmi (IMC)\n",
        "sns.histplot(ax=axes[2], data=X_train_scaled, x='bmi', kde=True, color='red')\n",
        "axes[2].set_ylabel('Frequência')\n",
        "axes[2].set_xlabel('IMC')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mwxeBH-1cq41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os histogramas mostram que a forma geral da distribuição foi preservada e os valores foram transformados para ter uma média próxima de zero e um desvio padrão de um, o que era esperado após a padronização."
      ],
      "metadata": {
        "id": "EHwDmMYAcu1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica a transformação no conjunto de teste\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Converte e define dataframe que será usado como conjunto de teste\n",
        "X_test_scaled = pd.DataFrame(X_test_processed, columns=processed_cols)\n",
        "\n",
        "# Exibe conjunto de teste\n",
        "X_test_scaled.head()"
      ],
      "metadata": {
        "id": "5e5dkFlscw_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ-_rlSEqveL"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr1qr0C06bxp"
      },
      "source": [
        "Optei por utilizar Regressão Logística como baseline por sua simplicidade e alta interpretabilidade, estabelecendo um benchmark de performance e complexidade. Ela define um \"piso de performance\" claro, que qualquer modelo mais sofisticado deve obrigatoriamente superar para justificar sua utilização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjJjE1fXsdXn"
      },
      "outputs": [],
      "source": [
        "# Baseline - Regressão Logística\n",
        "start_time = time.time() # inicia o cronômetro\n",
        "\n",
        "# Define a Baseline como Regressão Logística\n",
        "lr_baseline = LogisticRegression(random_state=SEED, max_iter=1000)\n",
        "lr_baseline.fit(X_train_scaled, y_train) # Treina a baseline no conjunto de treino\n",
        "\n",
        "training_time = time.time() - start_time # finaliza e calcula o tempo\n",
        "\n",
        "# Previsões\n",
        "y_pred_baseline = lr_baseline.predict(X_test_scaled)\n",
        "y_proba_baseline = lr_baseline.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Métricas\n",
        "baseline_time = time.time() - start_time\n",
        "print(f\"Tempo de treino (baseline): {baseline_time:.2f} segundos\")\n",
        "print(\"\\nRelatório de classificação (Baseline - Regressão Logística):\")\n",
        "print(classification_report(y_test, y_pred_baseline, zero_division=0))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, y_proba_baseline):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW9FJwKY0Oag"
      },
      "source": [
        "A criação de uma baseline, treinada propositalmente nos dados de treino originais e desbalanceados serve como um benchmark mínimo e realista para o projeto. O resultado para a classe minoritária é, na verdade, um diagnóstico poderoso: ele confirma a gravidade do desbalanceamento, expõe a inutilidade de um modelo ingênuo que apenas aprende a prever o resultado mais comum, e estabelece que qualquer modelo só terá valor real se superar significativamente essa performance, principalmente ao alcançar um recall positivo para os casos de AVC. Usar essa abordagem, em vez de uma baseline artificialmente otimizada com dados já balanceados, permite mensurar o verdadeiro valor agregado pelas técnicas de modelagem e balanceamento subsequentes, garantindo que qualquer melhoria seja genuína e não uma ilusão criada por uma referência fraca."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq8uEGlaYxRm"
      },
      "source": [
        "##Balanceamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijMWio9cyMhE"
      },
      "source": [
        "A análise exploratória revelou um severo desbalanceamento de classes no dataset, onde a classe minoritária (stroke=1) representa apenas 1,8% do total de registros. Diante deste cenário, é necessário utilizar alguma técnica de balanceamento para evitar um modelo com acurácia falsamente alta prevendo somente a não ocorrência de AVC para todos os pacientes. Optei por utilizar as técnicas SMOTE, UnderSampling, e SMOTEENN e avaliar como os modelos se comportam em cada uma delas. Abaixo é possível verificar a distribuição das clases após a aplicação de cada uma das técnicas aplicadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AtAAntKsGpt"
      },
      "source": [
        "###SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTHbOLlZs1Jz"
      },
      "source": [
        "O **SMOTE** cria novos dados sintéticos, enriquecendo o dataset e ajudando o modelo a criar uma região de decisão mais ampla e generalista para a classe minoritária, reduzindo o risco de overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVq8K9EUhntR"
      },
      "outputs": [],
      "source": [
        "# Balanceamento de Classes usando SMOTE no conjunto de treino somente\n",
        "print(\"Distribuição de classes antes do SMOTE:\", y_train.value_counts())\n",
        "\n",
        "smote = SMOTE(random_state=SEED)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nDistribuição de classes depois do SMOTE:\", y_train_smote.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XvFnpoiy2JE"
      },
      "source": [
        "Acima podemos ver que agora as classes estão balanceadas no conjunto de treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vyt7aZ6sJ85"
      },
      "source": [
        "###UnderSampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVhyAn3ms4e1"
      },
      "source": [
        "O **UnderSampling** tem a capacidade de balancear o dataset reduzindo o número de exemplos da classe majoritária, o que mitiga o viés do modelo em favor dessa classe e pode diminuir drasticamente o custo computacional e o tempo de treinamento. Por outro lado, sua maior desvantagem é o risco significativo de descartar informações importantes e úteis ao remover dados da classe majoritária, o que pode levar o modelo a não aprender os padrões de forma completa e, consequentemente, resultar em um desempenho de generalização inferior em dados reais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJQ0T1WJsOBu"
      },
      "outputs": [],
      "source": [
        "# Balanceamento de Classes usando UnderSampling no conjunto de treino somente\n",
        "print(\"Distribuição de classes antes do UnderSampling:\", y_train.value_counts())\n",
        "\n",
        "under_sampler = RandomUnderSampler(random_state=SEED)\n",
        "X_train_undersampled, y_train_undersampled = under_sampler.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nDistribuição de classes depois do UnderSampling:\", y_train_undersampled.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7-jkgGOtE-x"
      },
      "source": [
        "Acima podemos ver que agora as classes estão balanceadas no conjunto de treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWve-kSHALPY"
      },
      "source": [
        "###SMOTEENN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdeZn6Rs9KE"
      },
      "source": [
        "Essa técnica primeiro aplica o SMOTE para criar novas amostras e depois faz undersampling para remover amostras ruidosas ou ambíguas, \"limpando\" a fronteira de decisão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e28rFo-rANUz"
      },
      "outputs": [],
      "source": [
        "print(\"Distribuição de classes antes do SMOTEENN:\", y_train.value_counts())\n",
        "\n",
        "# Balanceamento de Classes usando SMOTEENN no conjunto de treino somente\n",
        "sampler = SMOTEENN(random_state=SEED)\n",
        "X_train_smoteenn, y_train_smoteenn = sampler.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nDistribuição de classes depois do SMOTEENN:\", y_train_smoteenn.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSQ5hwA4Ty-7"
      },
      "source": [
        "Acima podemos ver que agora as classes estão balanceadas no conjunto de treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqg72WyJy1xl"
      },
      "source": [
        "# Modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apycZwHMQYYS"
      },
      "source": [
        "Abaixo será executada uma série de testes, utilizando os modelos candidatos com o dataset original e com técnicas de balanceamento. Optei por comentar o algoritmo SVM, pois o tempo de processamento aumenta drasticamente e o resultado não é interessante.\n",
        "\n",
        "Na primeira célula com os modelos candidatos, os modelos serão treinados sem validação cruzada. Já na segunda, o código aplica a validação cruzada com 5 folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv3eslucRX8V"
      },
      "source": [
        "As métricas mais relevantes para esse projeto são a precisão, o recall e a f1-score. Isso se deve à importancia do modelo acertar os casos positivos de AVC alarmando o mínimo de pessoas saudáveis o possível. Ou seja, preciso do máximo de verdadeiros positivos com o mínimo de falsos positivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v882px_ac7B"
      },
      "outputs": [],
      "source": [
        "# 1. Dicionário com os modelos\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=SEED, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=SEED),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=SEED),\n",
        "    'NB': GaussianNB(),\n",
        "    'CART': DecisionTreeClassifier(),\n",
        "    #'SVM': SVC(random_state=SEED, probability=True), # Devido ao tempo de treinamento elevado e o resultado ruim, optei por comentar esse algoritmo\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# 2. Dicionário com as estratégias de balanceamento\n",
        "balancers = {\n",
        "    'Original': None, # Sem balanceamento\n",
        "    'SMOTE': SMOTE(random_state=SEED),\n",
        "    'UnderSample': RandomUnderSampler(random_state=SEED),\n",
        "    'SMOTEENN': SMOTEENN(random_state=SEED)\n",
        "}\n",
        "\n",
        "# 3. Definição do passo de pré-processamento\n",
        "vcontinuas = ['age', 'avg_glucose_level', 'bmi']\n",
        "preprocessor = ColumnTransformer(transformers=[('scaler_continuas', StandardScaler(), vcontinuas)],remainder='passthrough')\n",
        "\n",
        "# 4. Lista para armazenar todos os resultados\n",
        "results_list = []\n",
        "\n",
        "# ----------- Loop de experimentação com Pipeline ---------\n",
        "\n",
        "for bal_name, balancer in balancers.items():\n",
        "    print(f\"\\n{'='*20} Processando Estratégia: {bal_name.upper()} {'='*20}\")\n",
        "\n",
        "    # Loop para treinar e avaliar cada modelo com a estratégia atual\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"--- Treinando {model_name} ---\")\n",
        "\n",
        "        # Cria um pipeline completo para cada combinação\n",
        "        steps = [('preprocessor', preprocessor)]\n",
        "        if balancer:\n",
        "            steps.append(('sampler', balancer))\n",
        "        steps.append(('model', model))\n",
        "\n",
        "        pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "        # Inicia o cronômetro\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Treina o modelo com pipeline nos dados\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Finaliza e calcula o tempo\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Faz previsões no conjunto de Teste\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "\n",
        "        # Obtem probabilidades para o AUC-ROC. Trata exceções para modelos que não têm predict_proba.\n",
        "        try:\n",
        "            y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "        except AttributeError:\n",
        "            y_proba = y_pred # Para modelos como SVM sem probability=True\n",
        "\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "        results_list.append({\n",
        "            'Estrategia': bal_name,\n",
        "            'Modelo': model_name,\n",
        "            'Precisao (AVC)': report.get('1', {}).get('precision', 0),\n",
        "            'Recall (AVC)':   report.get('1', {}).get('recall', 0),\n",
        "            'F1-Score (AVC)': report.get('1', {}).get('f1-score', 0),\n",
        "            'AUC-ROC':        roc_auc_score(y_test, y_proba),\n",
        "            'Tempo (s)':      training_time\n",
        "        })\n",
        "\n",
        "        print(f\"Tempo de Treino: {training_time:.2f} segundos\")\n",
        "\n",
        "\n",
        "# --- Visualização dos resultados ---\n",
        "\n",
        "# Cria o DataFrame final com todos os resultados\n",
        "results_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Ordena o DataFrame para ver facilmente os melhores resultados\n",
        "sorted_results = results_df.sort_values(by=['Precisao (AVC)','Recall (AVC)'], ascending=False)\n",
        "\n",
        "# Exibe os resultados\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"TABELA DE COMPARAÇÃO DE MODELOS\")\n",
        "print(\"=\"*50)\n",
        "print(sorted_results.round(2)) # Arredonda para 2 casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_MrPDIQtq6J"
      },
      "outputs": [],
      "source": [
        "# 1. Dicionário com os modelos\n",
        "#models = {\n",
        "    #'Logistic Regression': LogisticRegression(random_state=SEED, max_iter=1000),\n",
        "    #'Random Forest': RandomForestClassifier(random_state=SEED),\n",
        "    #'Gradient Boosting': GradientBoostingClassifier(random_state=SEED),\n",
        "    #'NB': GaussianNB(),\n",
        "    #'CART': DecisionTreeClassifier(),\n",
        "    #'SVM': SVC(random_state=SEED, probability=True), # Devido ao tempo de treinamento elevado e o resultado ruim, optei por comentar esse algoritmo\n",
        "    #'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "#}\n",
        "\n",
        "# 2. Dicionário com as estratégias de balanceamento\n",
        "#balancers = {\n",
        "    #'Original': None, # Sem balanceamento\n",
        "    #'SMOTE': SMOTE(random_state=SEED),\n",
        "    #'UnderSample': RandomUnderSampler(random_state=SEED),\n",
        "    #'SMOTEENN': SMOTEENN(random_state=SEED)\n",
        "#}\n",
        "\n",
        "# 3. Lista para armazenar todos os resultados\n",
        "#results_list = []\n",
        "\n",
        "# Usa 5 folds estratificados para processamento mais rápido\n",
        "#cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# ----------- Loop de experimentação ---------\n",
        "#for bal_name, balancer in balancers.items():\n",
        "    #print(f\"\\n{'='*20} Processando Estratégia: {bal_name.upper()} {'='*20}\")\n",
        "\n",
        "    #for model_name, model in models.items():\n",
        "        #print(f\"--- Avaliando {model_name} com Validação Cruzada ---\")\n",
        "\n",
        "        # # Inicia o cronômetro\n",
        "        #start_time = time.time()\n",
        "\n",
        "        # Cria um pipeline completo para cada combinação\n",
        "        #steps = [('preprocessor', preprocessor)]\n",
        "        #if balancer:\n",
        "            #steps.append(('sampler', balancer))\n",
        "        #steps.append(('model', model))\n",
        "\n",
        "        #pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "        # Calcula os scores de Recall\n",
        "        #recall_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_strategy, scoring=make_scorer(recall_score, pos_label=1), n_jobs=-1)\n",
        "\n",
        "        # Calcula os scores de Precisão\n",
        "        #precision_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_strategy, scoring=make_scorer(precision_score, pos_label=1, zero_division=0), n_jobs=-1)\n",
        "\n",
        "        # Finaliza e calcula o tempo\n",
        "        #training_time = time.time() - start_time\n",
        "\n",
        "        # Salva a MÉDIA dos resultados da validação cruzada\n",
        "        #results_list.append({\n",
        "            #'Estrategia': bal_name,\n",
        "            #'Modelo': model_name,\n",
        "            #'Recall Médio (CV)': recall_cv_scores.mean(),\n",
        "            #'Precisao Média (CV)': precision_cv_scores.mean(),\n",
        "            #'Tempo (s)': training_time\n",
        "        #})\n",
        "\n",
        "        #print(f\"Tempo de Treino: {training_time:.2f} segundos\")\n",
        "\n",
        "# --- Visualização dos resultados ---\n",
        "\n",
        "# Cria o DataFrame final com todos os resultados\n",
        "#results_df = pd.DataFrame(results_list)\n",
        "# Renomeia a variável para evitar conflito com o DataFrame original\n",
        "#sorted_results_cv = results_df.sort_values(by=['Precisao Média (CV)','Recall Médio (CV)'], ascending=False)\n",
        "\n",
        "# Exibe os resultados\n",
        "#print(\"\\n\\n\" + \"=\"*50)\n",
        "#print(\"TABELA DE COMPARAÇÃO DE MODELOS (Validação Cruzada)\")\n",
        "#print(\"=\"*50)\n",
        "#print(sorted_results_cv.round(2)) # Arredonda para 2 casas decimais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMTEdUn-PsVF"
      },
      "source": [
        "Na célula acima é executado o treinamento dos modelos candidatos com e sem as técnicas de balanceamento e utilizando a validação cruzada com 5 folds. Devido ao tempo de execução, próximo aos 10 minutos, optei por deixar o código comentado e abaixo uma tabela resumo com os modelos que apresentaram melhor resultado. Podemos ver mais uma vez que os resultados sofreram pouca alteração, tendo no geral uma precisão pior.\n",
        "\n",
        "---\n",
        "\n",
        "| Estrategia | Modelo | Recall Médio (CV) | Precisao Média (CV) | Tempo de treinamento (s) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| Original | NB | 1.00 | 0.02 | 0.54 |\n",
        "| UnderSample | NB | 1.00 | 0.02 | 0.50 |\n",
        "| SMOTEENN | NB | 1.00 | 0.02 | 38.55 |\n",
        "| SMOTE | NB | 1.00 | 0.02 | 0.73 |\n",
        "| SMOTEENN | Logistic Regression | 0.82 | 0.05 | 39.01 |\n",
        "| UnderSample | Random Forest | 0.82 | 0.05 | 2.82 |\n",
        "| UnderSample | Gradient Boosting | 0.81 | 0.05 | 2.48 |\n",
        "| SMOTE | Logistic Regression | 0.81 | 0.05 | 2.46 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udmH1L9AxOis"
      },
      "source": [
        "De forma geral, utilizar a validação cruzada não fez sentido no meu dataset, pois além do tempo de treinamento ser muito maior, o resultado foi inferior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GByWxLu4u7Mx"
      },
      "source": [
        "Os testes mostraram uma precisão baixíssima, o que indica que os modelos estão causando um alto número de falsos positivos. No contexto do meu problema, isso é ruim porque cria um alarme desnecessário em muitas pessoas que não terão AVC.\n",
        "\n",
        "No entanto, o recall é interessante, indicando que o modelo não é cego e consegue identificar os verdadeiros positivos de forma minimamente satisfatória.\n",
        "\n",
        "---\n",
        "\n",
        "Após o teste e compração de diversos modelos e utilizando diversas técnicas, a Regressão Logística com SMOTE se apresentou como mais promissora por aprensentar um melhor equilíbrio entre recall e precisão, com 0.06 e 0.81 respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLmzERwktucz"
      },
      "source": [
        "Essa análise inicial indicou modelos com precisão muito abaixo da aceitável, tornando o modelo quase inútil até mesmo para ser usado como uma triagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAI9loyJueLn"
      },
      "source": [
        "**Tentativa com Class Weigth no melhor modelo (Regressão Logística)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk_tfJPgE2Ym"
      },
      "outputs": [],
      "source": [
        "# ---- Avaliação com Pipeline ----\n",
        "\n",
        "# Componente A: O Pré-processador\n",
        "# Definido na etapa de padronização\n",
        "\n",
        "# Componente B: Modelo base que dará muito mais importância à classe minoritária\n",
        "lr_com_peso = LogisticRegression(class_weight='balanced',random_state=SEED)\n",
        "\n",
        "# --- Montagem do Pipeline ---\n",
        "pipeline_lr_weight = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', lr_com_peso)\n",
        "])\n",
        "\n",
        "# Treinamento do modelo\n",
        "pipeline_lr_weight.fit(X_train, y_train)\n",
        "\n",
        "# Avalia os resultados\n",
        "y_pred = pipeline_lr_weight.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ648N9DScuY"
      },
      "source": [
        "Aqui tentei utilizar o modelo com melhor resultado no dataset sem balanceamento, mas focando em dar mais peso para a classe desbalanceada. No entanto o resultado do recall foi um pouco pior, então desconsiderarei."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYBkrqAIq0kx"
      },
      "source": [
        "**Tentativa com ensembles Bagging Classifier e AdaBoostClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLTcls3Xol5l"
      },
      "outputs": [],
      "source": [
        "# ---- Avaliação com Pipeline ----\n",
        "\n",
        "# Componente A: O Pré-processador\n",
        "# Definido na etapa de padronização\n",
        "\n",
        "# Componente B: O Balanceador\n",
        "sampler = SMOTE(random_state=SEED) # Melhor balanceador encontrado para esse teste\n",
        "\n",
        "# Componente C: Modelo base de Regressão Logística\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Cria o ensemble Bagging com a Regressão Logística como estimador base\n",
        "bagging_model = BaggingClassifier(estimator=log_reg, n_estimators=50, random_state=SEED)\n",
        "\n",
        "# --- Montagem do Pipeline ---\n",
        "pipeline_bagging_model = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('sampler', sampler),\n",
        "    ('model', bagging_model)\n",
        "])\n",
        "\n",
        "# Treina o modelo ensemble\n",
        "start_time = time.time() # Inicia o cronômetro\n",
        "pipeline_bagging_model.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Tempo de Treino: {training_time:.2f} segundos\")\n",
        "\n",
        "# Faz previsões\n",
        "y_pred_bagging = pipeline_bagging_model.predict(X_test)\n",
        "\n",
        "# Avaliação\n",
        "print(classification_report(y_test, y_pred_bagging))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvD6tCZlru12"
      },
      "source": [
        "Acima podemos ver o resultado identico ao modelo de regressão logística mais simples, mas consumindo um tempo de treino muito maior, o que não justifica a utilização desse modelo mais complexo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyYiD3wZqe1g"
      },
      "outputs": [],
      "source": [
        "# ---- Avaliação com Pipeline ----\n",
        "\n",
        "# Componente A: O Pré-processador\n",
        "# Definido na etapa de padronização\n",
        "\n",
        "# Componente B: O Balanceador\n",
        "sampler = SMOTEENN(random_state=SEED) # Melhor balanceador encontrado para esse teste\n",
        "\n",
        "# Componente C: ensemble AdaBoost com a Regressão Logística como estimador base\n",
        "adaboost_model = AdaBoostClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=50, random_state=SEED)\n",
        "\n",
        "# --- Montagem do Pipeline ---\n",
        "pipeline_adaboost = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('sampler', sampler),\n",
        "    ('model', adaboost_model)\n",
        "])\n",
        "\n",
        "# Treina o modelo\n",
        "start_time = time.time() # Inicia o cronômetro\n",
        "pipeline_adaboost.fit(X_train, y_train)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Tempo de Treino: {training_time:.2f} segundos\")\n",
        "\n",
        "# Faz previsões\n",
        "y_pred_boost = pipeline_adaboost.predict(X_test)\n",
        "\n",
        "# Avaliação\n",
        "print(classification_report(y_test, y_pred_boost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfV89RTKr3hu"
      },
      "source": [
        "Utizando o AdaBoost o recall melhorou um pouco, mas a precisão caiu. O valor da precisão já está crítico, então não considerarei este modelo como viável."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpR4jHxF8ikv"
      },
      "source": [
        "**Rede Neural**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqv2zxsn8h8Z"
      },
      "outputs": [],
      "source": [
        "# ---- Construção do modelo ----\n",
        "\n",
        "# 1. Calcula o peso das classes\n",
        "# Isso dirá à rede neural para prestar mais atenção à classe minoritária (AVC)\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {0: weights[0], 1: weights[1]}\n",
        "\n",
        "print(f\"Pesos das classes: {class_weights}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1j_HZwk9Jhw"
      },
      "outputs": [],
      "source": [
        "# 2. Definir a arquitetura do modelo\n",
        "model = keras.Sequential([\n",
        "    # Camada de entrada - o shape deve ser o número de features do seu X\n",
        "    keras.Input(shape=(X_train_scaled.shape[1],)),\n",
        "\n",
        "    # Primeira camada oculta - 32 neurônios, com uma função de ativação comum 'relu'\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "\n",
        "    # Camada de Dropout para combater overfitting (boa prática)\n",
        "    keras.layers.Dropout(0.3),\n",
        "\n",
        "    # Segunda camada oculta\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "\n",
        "    # Camada de saída - 1 neurônio com ativação 'sigmoid' para classificação binária\n",
        "    # O sigmoid comprime a saída para um valor entre 0 e 1 (uma probabilidade)\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Mostra um resumo da arquitetura criada\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhbfN5qh-RFe"
      },
      "outputs": [],
      "source": [
        "# 3. Compila o modelo\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_02rWq7-bAc"
      },
      "outputs": [],
      "source": [
        "# Callback para parar o treino se não houver melhora, evitando overfitting\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_recall', # Monitorar o recall na validação\n",
        "    patience=10,          # Número de épocas sem melhora antes de parar\n",
        "    restore_best_weights=True # Restaura os pesos da melhor época\n",
        ")\n",
        "\n",
        "# 4. Treina o modelo\n",
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    epochs=100, # Número máximo de épocas\n",
        "    batch_size=32,\n",
        "    validation_split=0.2, # Usa 20% dos dados de treino para validação a cada época\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0qvdD3f-24S"
      },
      "outputs": [],
      "source": [
        "# ----- Teste da rede neural ----\n",
        "\n",
        "# 1. Obtem as probabilidades no conjunto de teste\n",
        "y_proba_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# 2. Aplica um threshold padrão de 0.5 para uma avaliação inicial\n",
        "y_pred_nn_default = (y_proba_nn > 0.5).astype(int)\n",
        "\n",
        "print(\"\\n--- Relatório da Rede Neural com Threshold Padrão (0.5) ---\")\n",
        "print(classification_report(y_test, y_pred_nn_default))\n",
        "\n",
        "# 3. Encontra e aplica o melhor threshold\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_nn)\n",
        "\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "f1_scores = f1_scores[:-1]\n",
        "best_f1_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_f1_idx]\n",
        "\n",
        "y_pred_nn_best = (y_proba_nn > best_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nMelhor Threshold encontrado (maximizando F1): {best_threshold:.4f}\")\n",
        "print(f\"\\n--- Relatório da Rede Neural com Threshold Otimizado ---\")\n",
        "print(classification_report(y_test, y_pred_nn_best))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pei6XkHQUtpA"
      },
      "source": [
        "Como uma última alternativa para tentar encontrar um modelo que consiga maior precisão sem reduzir tanto o recall, tentei aplicar uma rede neural simples para avaliar o resultado.\n",
        "\n",
        "Mais uma vez consegui uma melhora no recall, mas a precisão caiu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXreQLUj8clW"
      },
      "source": [
        "#Reavaliação da variável alvo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWO3d0iv9cHr"
      },
      "source": [
        "O resultado muito abaixo do esperado me fez voltar na análise exploratória, realizada na sprint anterior, e notei, através do gráfico abaixo que a maior concentração de casos de ocorrência de AVC está em pessoas com 60 anos ou mais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le6vwPKa8shh"
      },
      "outputs": [],
      "source": [
        "# Confirguração e título\n",
        "fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
        "plt.suptitle('Idade vs. Ocorrência de AVC', fontsize=16)\n",
        "\n",
        "# Boxplot 1: age (idade)\n",
        "sns.boxplot(ax=axes, hue='stroke', x='stroke', y='age', data=avcpreproc, palette={0: 'lightblue', 1: 'red'}, legend=False)\n",
        "axes.set_xlabel('')\n",
        "axes.set_ylabel('Idade')\n",
        "axes.set_xticks([0, 1])\n",
        "axes.set_xticklabels(['Não teve AVC', 'Teve AVC'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSF0pLNp9uCK"
      },
      "source": [
        "Decidi então aplicar um corte no dataset, mantendo somente os pacientes com 60 anos ou mais. Para não alongar muito mais o projeto, passarei por algumas etapas novamente de forma mais rápida, aplicando um pipeline final com os melhores resultados que encontrei."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9aThdJxUjZS"
      },
      "source": [
        "O objetivo desse corte é deixar o modelo focar no grupo de risco e reduzir o desbalanceamento. De 1,80%, a proporção da classe minoritária passou para 5,32%, que poderá ser verificado abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimentei também uma abordagem diferente para a padronização, e cheguei à conclusão que padronizar somente as variáveis ***age*** e ***avg_glucose_level*** me permitiram um resultado levemente superior."
      ],
      "metadata": {
        "id": "tnouSsHiXmJf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPz7zl4QTo4k"
      },
      "source": [
        "Ainda com o objetivo de não tornar o projeto muito maior do que se encontra, resumo através desse texto que realizei todos os testes novamente, com todas as técnicas de balanceamento vistas até aqui, com e sem validação cruzada. Nessa fase, o resultado mais interessante foi o NB utilizando UnderSample que atingiu precisão de 0.08 e recall de 0.75 com a validação cruzada com 5 folds.\n",
        "\n",
        "Também testei novamente o desempenho da mesma rede neural com esse dataset filtrado, o resultado foi semelhante.\n",
        "\n",
        "Mas o melhor resultado foi obtido através do uso do ensemble Voting com NB, Regressão Logística e Random Forest utilizando o dataset balanceado com UnderSample. Esse modelo será detalhado nas células seguintes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWvb4AGnWGd-"
      },
      "source": [
        "**- Filtrando e exibindo o novo dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYWqEZ8r-6f4"
      },
      "outputs": [],
      "source": [
        "# Filtra o DataFrame para manter as linhas onde a idade é maior ou igual a 60\n",
        "avcpreproc_filtered = avcpreproc[avcpreproc['age'] >= 60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THb1XeLz-9w6"
      },
      "outputs": [],
      "source": [
        "# Verifica o dataset filtrado\n",
        "avcpreproc_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4EBd4MXTMrA"
      },
      "outputs": [],
      "source": [
        "# Verificando as dimensões do dataset filtrado\n",
        "print(f\"Dimensões do Dataset: {avcpreproc_filtered.shape}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U81YNZhGXvXP"
      },
      "source": [
        "O dataset reduziu consideralvemente. Agora possui 32.262 linhas a menos.\n",
        "\n",
        "As linhas restantes podem ajudar o modelo a diferenciar melhor o grupo de risco uma vez que todos os ruídos gerados pelas informações dos pacientes com menos de 60 anos foram removidos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Separação entre treino e teste no dataset filtrado**"
      ],
      "metadata": {
        "id": "LK51C4k1uvzx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn8wXnT__NO0"
      },
      "outputs": [],
      "source": [
        "# Separação dos Dados (Features e Target)\n",
        "X = avcpreproc_filtered.drop('stroke', axis=1)\n",
        "y = avcpreproc_filtered['stroke']\n",
        "\n",
        "# Divisão estratificada em conjuntos de treino e teste para manter a proporção\n",
        "X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X, y ,test_size=0.3, random_state=SEED, stratify = y)\n",
        "\n",
        "print('Tamanho das separações:')\n",
        "print(f\"Treino: {X_train_filtered.shape}, {y_train_filtered.shape}\")\n",
        "print(f\"Teste: {X_test_filtered.shape}, {y_test_filtered.shape}\")\n",
        "print(f\"Proporção de stroke no treino: {y_train_filtered.mean():.4f}\")\n",
        "print(f\"Proporção de stroke no teste: {y_test_filtered.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTw94TwwsXk6"
      },
      "source": [
        "##Modelagem e otimização de hiperparâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AZBf_rBUF3Q"
      },
      "source": [
        "Na tentativa abaixo, foi utilizado o ensemble Voting com NB, Regressão Logística e Random Forest, como última tentativa da melhorar os resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Avaliação com Pipeline ----\n",
        "\n",
        "# Componente A: O Pré-processador\n",
        "# Define quais colunas serão padronizadas\n",
        "vcontinuas = ['age', 'avg_glucose_level'] # Dessa vez a padronização não ocorrerá na coluna bmi\n",
        "preprocessor = ColumnTransformer(transformers=[('scaler_continuas', StandardScaler(), vcontinuas)],remainder='passthrough')\n",
        "\n",
        "# Componente B: O Balanceador\n",
        "sampler = RandomUnderSampler(random_state=SEED) # Melhor balanceador encontrado\n",
        "\n",
        "# Componente C: ensemble Voting como estimador base\n",
        "clf_nb = GaussianNB()\n",
        "clf_lr = LogisticRegression(random_state=SEED, solver='liblinear', max_iter=1000)\n",
        "clf_rf = RandomForestClassifier(random_state=SEED)\n",
        "\n",
        "# Cria o ensemble Voting\n",
        "voting_model = VotingClassifier(estimators=[('nb', clf_nb), ('lr', clf_lr), ('rf', clf_rf)],voting='soft')\n",
        "\n",
        "# --- Montagem do Pipeline ---\n",
        "pipeline_voting = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('sampler', sampler),\n",
        "    ('model', voting_model)\n",
        "])\n",
        "\n",
        "# Treinar e avaliar\n",
        "pipeline_voting.fit(X_train_filtered, y_train_filtered)\n",
        "y_pred = pipeline_voting.predict(X_test_filtered)\n",
        "print(classification_report(y_test_filtered, y_pred))"
      ],
      "metadata": {
        "id": "r8zRhaNINXj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWWKOKkJUQ_9"
      },
      "source": [
        "Como podemos ver, finalmente conseguimos uma melhora na precisão, com o valor atingido não sendo visto em nenhum teste anterior, além disso, o recall manteve um patamar mínimo aceitável."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBahKcI0qhKq"
      },
      "source": [
        "### Otimização dos hiperparâmetros utilizando GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Padroniza\n",
        "# Aplica o pré-processador\n",
        "#X_train_scaled_filtered = preprocessor.fit_transform(X_train_filtered)\n",
        "# Aplica a transformação no conjunto de teste\n",
        "#X_test_scaled_filtered = preprocessor.transform(X_test_filtered)\n",
        "\n",
        "#2. Aplica o balanceamento\n",
        "# Balanceamento de Classes usando UnderSampling no conjunto de treino somente\n",
        "#X_train_undersampled_filtered, y_train_undersampled_filtered = sampler.fit_resample(X_train_scaled_filtered, y_train_filtered)\n",
        "\n",
        "# 3. Define o Grid de Hiperparametros\n",
        "#param_grid = {\n",
        "    #'lr__C': [0.1, 1.0, 10.0],  # Testa diferentes valores de regularização para a Regressão Logística\n",
        "    #'lr__penalty': ['l1', 'l2'],\n",
        "    #'lr__solver': ['liblinear', 'saga'],\n",
        "    #'rf__n_estimators': [50, 100, 200], # Testa diferentes números de árvores para o Random Forest\n",
        "    #'rf__max_depth': [None, 10, 20],   # Testa diferentes profundidades de árvore\n",
        "    #'weights': [[1, 1, 1], [1, 2, 1], [1, 1, 2]] # Testa diferentes pesos de voto para os modelos\n",
        "#}\n",
        "# GaussianNB não tem hiperparâmetros importantes para serem otimizados, por isso o deixei de fora.\n",
        "\n",
        "# 4. Configura o GridSearch\n",
        "#    A métrica de otimização será o 'f1' para buscar um bom equilíbrio\n",
        "#cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED) # Usei 5 splits para ser mais rápido e porque 10 não trouxe melhorias em outro teste\n",
        "\n",
        "#grid_search = GridSearchCV(\n",
        "    #estimator=voting_model, # O estimador foi definido na célula anterior\n",
        "    #param_grid=param_grid,\n",
        "    #scoring='f1', # Otimiza para o F1-Score\n",
        "    #cv=cv_strategy,\n",
        "    #n_jobs=-1,\n",
        "    #verbose=2\n",
        "#)\n",
        "\n",
        "#print(\"Iniciando a otimização de hiperparâmetros para o VotingClassifier...\")\n",
        "# 5. Executa a busca\n",
        "#    Treina com os dados de treino com UnderSample\n",
        "#grid_search.fit(X_train_undersampled_filtered, y_train_undersampled_filtered)\n",
        "\n",
        "# 6. Analie dos resultados\n",
        "#print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
        "#print(grid_search.best_params_)\n",
        "\n",
        "#print(f\"\\nMelhor score F1 (ponderado) da validação cruzada: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 7. Avalia o melhor modelo no conjunto de teste\n",
        "#print(\"\\nAvaliação do melhor modelo encontrado no conjunto de teste:\")\n",
        "#best_voting_model = grid_search.best_estimator_\n",
        "#y_pred = best_voting_model.predict(X_test_scaled_filtered)\n",
        "#print(classification_report(y_test_filtered, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "dlY25JVFcpez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Oidq2IpOYg"
      },
      "source": [
        "Devido ao tempo para execução da célula, superior a 7 minutos, optei por deixar o código comentado, mas deixando o resultado abaixo.\n",
        "\n",
        "Melhores hiperparâmetros encontrados:\n",
        "`{'lr__C': 0.1, 'lr__penalty': 'l1', 'lr__solver': 'saga', 'rf__max_depth': 10, 'rf__n_estimators': 50, 'weights': [1, 2, 1]}`\n",
        "\n",
        "Métricas encontradas:\n",
        "\n",
        "| Target | Precision | Recall | F1-score | Support |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| 0 | 0.98 | 0.59 | 0.74 | 3161 |\n",
        "| 1 | 0.10 | 0.77 | 0.17 | 178 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pipeline e teste final"
      ],
      "metadata": {
        "id": "ZI2beZV7Y40N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Pipeline final**"
      ],
      "metadata": {
        "id": "FYKUJGXtmPn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Componente A: O Pré-processador\n",
        "# Define quais colunas serão padronizadas\n",
        "vcontinuas = ['age', 'avg_glucose_level'] # Dessa vez a padronização não ocorrerá na coluna bmi\n",
        "preprocessor = ColumnTransformer(transformers=[('scaler_continuas', StandardScaler(), vcontinuas)],remainder='passthrough')\n",
        "\n",
        "# Componente B: O Balanceador\n",
        "sampler = RandomUnderSampler(random_state=SEED) # Melhor balanceador encontrado\n",
        "\n",
        "# Componente C: O Modelo\n",
        "# Modelo base com hiperparâmetros ajustados\n",
        "clf_nb = GaussianNB()\n",
        "clf_lr = LogisticRegression(random_state=SEED,solver='saga', penalty='l1', C=0.1, max_iter=1000)\n",
        "clf_rf = RandomForestClassifier(random_state=SEED,n_estimators=50, max_depth=10)\n",
        "\n",
        "# Cria o ensemble Voting com hiperparâmetros ajustados\n",
        "best_model = VotingClassifier(estimators=[('nb', clf_nb), ('lr', clf_lr), ('rf', clf_rf)], voting='soft', weights=[1, 2, 1])\n",
        "\n",
        "# --- Montagem do Pipeline Final ---\n",
        "final_pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('sampler', sampler),\n",
        "    ('model', best_model)\n",
        "])"
      ],
      "metadata": {
        "id": "vFtfLdokYsQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjT6aj6kWThF"
      },
      "source": [
        "O pipeline final foi definido através de intensos testes. Com o dataset filtrado e padronizado dessa forma, o balancemaneto com UnderSample foi o que trouxe melhores resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Teste final**"
      ],
      "metadata": {
        "id": "ZjVkUR3ymTtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste e avaliação finais\n",
        "final_pipeline.fit(X_train_filtered, y_train_filtered)\n",
        "y_pred_best = final_pipeline.predict(X_test_filtered)\n",
        "print(classification_report(y_test_filtered, y_pred_best))"
      ],
      "metadata": {
        "id": "Z3cYjR2MfdPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JIN996W14hC"
      },
      "source": [
        "Após exaustivos testes, utilizando diversos modelos e configurações, o modelo que obteve melhor desempenho foi o ensemble Voting utilizando o dataset padronizado e com UnderSample, devido ao recall mais elevado na classe onde ocorre o AVC e à precisão que foi mais alta. Isso aliado ao tempo de treinamento indica ser o melhor modelo para seguir no momento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeAjmBXBwZV-"
      },
      "source": [
        "\n",
        "# Avaliação final, análise de erros e limitações"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Melhor modelo vs Baseline**"
      ],
      "metadata": {
        "id": "0Q5PDDilq1Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Resultado da baseline:')\n",
        "print(classification_report(y_test, y_pred_baseline, zero_division=0))\n",
        "\n",
        "print('Resultado do melhor modelo:')\n",
        "print(classification_report(y_test_filtered, y_pred_best))"
      ],
      "metadata": {
        "id": "PhMAS662tFyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise comparativa demonstra um avanço significativo e indiscutível do \"Melhor Modelo\" sobre a baseline, pois ele foi capaz de aprender a identificar a classe minoritária, alcançando um recall de 77% para os casos de AVC, uma tarefa em que a baseline falhou completamente com 0%.\n",
        "\n",
        "Isso é um salto gigantesco e a prova de que o modelo aprendeu um padrão preditivo valioso.\n",
        "\n",
        "Essa conquista representa a transição de um modelo inútil para um com valor prático, que estabelece um sinal preditivo real, ainda que a baixa precisão de 10% mostre a necessidade de refinamento. O custo para esse ganho foi uma queda esperada na performance da classe majoritária, confirmando que o modelo está fazendo um trade-off ativo, e o desafio agora é aprimorar sua confiabilidade, melhorando a precisão sem sacrificar significativamente o recall já obtido."
      ],
      "metadata": {
        "id": "cjfXjiHRt91X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Matriz de confusão e análise de erros**"
      ],
      "metadata": {
        "id": "195uv7Tuu7-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqzUoccJC2UN"
      },
      "outputs": [],
      "source": [
        "# Cria a matriz de confusão\n",
        "cm = confusion_matrix(y_test_filtered, y_pred_best)\n",
        "print(\"\\nMatriz de confusão:\")\n",
        "print(cm)\n",
        "\n",
        "# Cria o gráfico com Seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "# Adiciona rótulos\n",
        "plt.xlabel('Rótulo Previsto')\n",
        "plt.ylabel('Rótulo Verdadeiro')\n",
        "\n",
        "# Mostra o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise da matriz de confusão revela um modelo que foi bem-sucedido em sua principal tarefa de detecção, alcançando um recall de 77%, o que significa que ele identificou corretamente a maioria dos casos reais de AVC e minimizou o erro mais crítico (Falsos Negativos). No entanto, essa alta sensibilidade teve um custo significativo na confiabilidade, evidenciado pela baixíssima precisão, resultado de um número excessivo de 1300 alarmes falsos (Falsos Positivos). Isso caracteriza o modelo como um detector \"sensível, mas pouco confiável\", cujo desafio agora é refinar sua capacidade de discernimento para aumentar a precisão sem comprometer drasticamente sua capacidade de detecção."
      ],
      "metadata": {
        "id": "IaDUDnwfwW26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Limitações**"
      ],
      "metadata": {
        "id": "ef7PsbsZx_rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A característica mais dominante e definidora do projeto é que os dados são extremamente desbalanceados, com a classe de AVC sendo muito rara. Mais importante ainda, a extensa experimentação provou que os atributos disponíveis, embora relevantes, possuem um sinal preditivo fraco. Isso significa que não há uma combinação clara e forte de características que separe inequivocamente os pacientes que terão um AVC dos que não terão. A informação está \"diluída\" em muito ruído, tornando a tarefa de classificação inerentemente difícil.\n",
        "\n",
        "Rapidamente notei que a acurácia é uma métrica perigosa e enganosa para este problema, pois uma baseline que sempre prevê \"não AVC\" alcança uma acurácia altíssima, mas é completamente inútil. A verdadeira história do projeto foi contada pelo conflito direto entre a Precisão e o Recall da classe 1 (AVC). Todas as tentativas de aumentar o Recall, seja com balanceamento ou modelos complexos, resultaram em uma queda drástica da Precisão. As métricas, portanto, não apenas mediram a performance, mas também serviram como uma ferramenta de diagnóstico, revelando consistentemente que o modelo não conseguia ser sensível e confiável ao mesmo tempo com os dados atuais.\n",
        "\n",
        "O projeto começou com um modelo de baseline que tinha um viés massivo para a classe majoritária, aprendendo a ignorar completamente a classe minoritária. Todas as técnicas subsequentes (SMOTE, UnderSampling, class_weight) foram esforços para combater esse viés e forçar os modelos a prestar atenção nos casos de AVC. Embora isso tenha sido bem-sucedido em aumentar o Recall, introduziu um novo padrão de erro: uma tendência a gerar Falsos Positivos. Essencialmente, trocamos um viés de \"omissão\" por um viés de \"excesso de alerta\", um passo necessário, mas que evidencia a dificuldade de encontrar um ponto de operação verdadeiramente sem viés.\n",
        "\n",
        "A conclusão é que, mesmo com as melhores práticas, a fraca capacidade de generalização dos modelos está diretamente ligada à fraqueza do sinal preditivo nos dados de origem."
      ],
      "metadata": {
        "id": "BPoJILkYx-1Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj1IZbpqwm3F"
      },
      "source": [
        "# Conclusões e próximos passos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As hipóteses iniciais foram postas à prova e o projeto foi bem-sucedido em desenvolver um modelo significativamente superior à baseline, com a capacidade de detectar a maioria dos casos de AVC ao atingir um alto recall após a aplicação de diversas técnicas. Contudo, essa sensibilidade foi alcançada ao custo de uma baixa precisão, um trade-off imposto pela natureza desbalanceada e pelo sinal preditivo fraco dos dados, onde mesmo algoritmos complexos e otimizados não conseguiram generalizar de forma eficaz. A conclusão definitiva é que o projeto atingiu um teto de performance imposto pelos atributos atuais, e as melhorias futuras mais promissoras não virão de novos modelos, mas sim de um foco estratégico na aquisição de novos dados para a classe minoritária, na incorporação de novas variáveis de entrada e na engenharia de atributos para enriquecer a informação disponível e, consequentemente, aprimorar a precisão do modelo sem sacrificar drasticamente sua já robusta capacidade de detecção."
      ],
      "metadata": {
        "id": "GCHuf1o40Rld"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfb/GFHgJNVuMLKXkpEwRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
